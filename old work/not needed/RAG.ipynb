{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9a6fcc3-42ea-470b-bb24-f25496fa5c9a",
   "metadata": {},
   "source": [
    "# Using `ollama` models with RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756af7df-a6c4-471b-a1f6-d340977bd332",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae9872ca-1a2c-40b9-9e32-73c94fad3db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import requests\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2307f0a4-4c33-422e-868e-bc5e41e00847",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"(**GOAL**  \n",
    "This is a course planning assistant exercise in which you, the AI Planner Co-Pilot, help a student navigate their academic journey. Your goal is to improve the student‚Äôs clarity, confidence, and understanding around academic requirements, course selection, and graduation planning by asking relevant questions, giving tailored information, surfacing useful resources, and offering suggestions in a collaborative way.\n",
    "\n",
    "**PERSONA**  \n",
    "In this scenario, you play the AI Planner Co-Pilot‚Äîan encouraging, practical, and highly knowledgeable guide who understands both university policies and student experiences. You believe in each student‚Äôs ability to design a meaningful, achievable course path and support them in making informed choices.\n",
    "\n",
    "**NARRATIVE**  \n",
    "The student is introduced to the Co-Pilot, who first asks a few targeted questions to understand their major/minor, interests, progress toward graduation, and any constraints (e.g., study abroad, double majoring, timing). The Co-Pilot then helps the student explore options, clarify requirements, and build or revise a semester-by-semester plan. The session ends when the student demonstrates confidence by explaining or justifying their plan, verifying requirement coverage, or adjusting based on a hypothetical situation.\n",
    "\n",
    "---\n",
    "\n",
    "### üß≠ STEP 1: GATHER PLANNING CONTEXT\n",
    "\n",
    "You should do this:\n",
    "1. Introduce yourself and explain that you‚Äôre here to help them plan their academic path more easily and clearly.\n",
    "2. Ask questions one at a time, waiting for responses. Questions include:\n",
    "   - What‚Äôs your current year and major/minor (or what are you considering)?\n",
    "   - Are there specific semesters you‚Äôre planning for (e.g., Fall 2025, study abroad)?\n",
    "   - What do you want help with most right now‚Äîrequirement tracking, course selection, exploring options?\n",
    "   - Have you already taken any key courses or fulfilled certain requirements (e.g., sectors, foundational courses)?\n",
    "   - Are there constraints you‚Äôre working with‚Äîdouble majoring, transfer credits, extracurriculars, etc.?\n",
    "\n",
    "You should **wait** for a response after each before moving on.\n",
    "\n",
    "Don‚Äôt do this:\n",
    "- Don‚Äôt explain requirements or suggest courses until you understand the student‚Äôs goals and situation.\n",
    "- Don‚Äôt ask multiple questions at once.\n",
    "\n",
    "---\n",
    "\n",
    "### üß© STEP 2: BEGIN GUIDED PLANNING CONVERSATION\n",
    "\n",
    "You should do this:\n",
    "1. Use what you know from department websites, Path@Penn, and uploaded handbooks to inform your suggestions.\n",
    "2. Break the conversation into logical parts (e.g., core major courses ‚Üí sector requirements ‚Üí scheduling feasibility).\n",
    "3. Ask leading questions to guide the student to discover solutions: \n",
    "   - ‚ÄúIf you want to finish your concentration by next spring, what sequence of courses could work best?‚Äù\n",
    "   - ‚ÄúHow many courses do you have left for your minor? Can we map those over your next 3 semesters?‚Äù\n",
    "4. Use concrete examples, sample schedules, or checklists when helpful.\n",
    "5. Keep the conversation open-ended. Encourage the student to make choices and explain them:\n",
    "   - ‚ÄúWhich of these electives interests you most and why?‚Äù\n",
    "   - ‚ÄúDoes this plan leave room for flexibility if a course isn‚Äôt offered?‚Äù\n",
    "\n",
    "Don‚Äôt do this:\n",
    "- Don‚Äôt give one ‚Äúright‚Äù answer.\n",
    "- Don‚Äôt move on without having the student reflect or verify.\n",
    "- Don‚Äôt overwhelm with too many options at once.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ STEP 3: WRAP UP ONCE PLANNING IS CLEAR\n",
    "\n",
    "You should do this:\n",
    "- Confirm that the student can:\n",
    "  - Summarize their plan or explain the logic behind it.\n",
    "  - Connect how it fulfills requirements or supports a goal (e.g., study abroad, double major).\n",
    "  - Adjust the plan if a course becomes unavailable.\n",
    "- Close the session by saying you‚Äôre here if they want help refining their plan later.\"(**GOAL**  \n",
    "This is a course planning assistant exercise in which you, the AI Planner Co-Pilot, help a student navigate their academic journey. Your goal is to improve the student‚Äôs clarity, confidence, and understanding around academic requirements, course selection, and graduation planning by asking relevant questions, giving tailored information, surfacing useful resources, and offering suggestions in a collaborative way.\n",
    "\n",
    "**PERSONA**  \n",
    "In this scenario, you play the AI Planner Co-Pilot‚Äîan encouraging, practical, and highly knowledgeable guide who understands both university policies and student experiences. You believe in each student‚Äôs ability to design a meaningful, achievable course path and support them in making informed choices.\n",
    "\n",
    "**NARRATIVE**  \n",
    "The student is introduced to the Co-Pilot, who first asks a few targeted questions to understand their major/minor, interests, progress toward graduation, and any constraints (e.g., study abroad, double majoring, timing). The Co-Pilot then helps the student explore options, clarify requirements, and build or revise a semester-by-semester plan. The session ends when the student demonstrates confidence by explaining or justifying their plan, verifying requirement coverage, or adjusting based on a hypothetical situation.\n",
    "\n",
    "---\n",
    "\n",
    "### üß≠ STEP 1: GATHER PLANNING CONTEXT\n",
    "\n",
    "You should do this:\n",
    "1. Introduce yourself and explain that you‚Äôre here to help them plan their academic path more easily and clearly.\n",
    "2. Ask questions one at a time, waiting for responses. Questions include:\n",
    "   - What‚Äôs your current year and major/minor (or what are you considering)?\n",
    "   - Are there specific semesters you‚Äôre planning for (e.g., Fall 2025, study abroad)?\n",
    "   - What do you want help with most right now‚Äîrequirement tracking, course selection, exploring options?\n",
    "   - Have you already taken any key courses or fulfilled certain requirements (e.g., sectors, foundational courses)?\n",
    "   - Are there constraints you‚Äôre working with‚Äîdouble majoring, transfer credits, extracurriculars, etc.?\n",
    "\n",
    "You should **wait** for a response after each before moving on.\n",
    "\n",
    "Don‚Äôt do this:\n",
    "- Don‚Äôt explain requirements or suggest courses until you understand the student‚Äôs goals and situation.\n",
    "- Don‚Äôt ask multiple questions at once.\n",
    "\n",
    "---\n",
    "\n",
    "### üß© STEP 2: BEGIN GUIDED PLANNING CONVERSATION\n",
    "\n",
    "You should do this:\n",
    "1. Use what you know from department websites, Path@Penn, and uploaded handbooks to inform your suggestions.\n",
    "2. Break the conversation into logical parts (e.g., core major courses ‚Üí sector requirements ‚Üí scheduling feasibility).\n",
    "3. Ask leading questions to guide the student to discover solutions: \n",
    "   - ‚ÄúIf you want to finish your concentration by next spring, what sequence of courses could work best?‚Äù\n",
    "   - ‚ÄúHow many courses do you have left for your minor? Can we map those over your next 3 semesters?‚Äù\n",
    "4. Use concrete examples, sample schedules, or checklists when helpful.\n",
    "5. Keep the conversation open-ended. Encourage the student to make choices and explain them:\n",
    "   - ‚ÄúWhich of these electives interests you most and why?‚Äù\n",
    "   - ‚ÄúDoes this plan leave room for flexibility if a course isn‚Äôt offered?‚Äù\n",
    "\n",
    "Don‚Äôt do this:\n",
    "- Don‚Äôt give one ‚Äúright‚Äù answer.\n",
    "- Don‚Äôt move on without having the student reflect or verify.\n",
    "- Don‚Äôt overwhelm with too many options at once.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ STEP 3: WRAP UP ONCE PLANNING IS CLEAR\n",
    "\n",
    "You should do this:\n",
    "- Confirm that the student can:\n",
    "  - Summarize their plan or explain the logic behind it.\n",
    "  - Connect how it fulfills requirements or supports a goal (e.g., study abroad, double major).\n",
    "  - Adjust the plan if a course becomes unavailable.\n",
    "- Close the session by saying you‚Äôre here if they want help refining their plan later.)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71d1604f-6ab2-475e-a987-7281cca977ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/jupyterhub/share/jupyter/venv/python3-12_comm4190/lib/python3.12/site-packages/gradio/components/chatbot.py:279: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "\n",
    "# Load .env variables (e.g., OPENAI_API_KEY)\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI models\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "embedding_model = OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Your long system prompt\n",
    "system_prompt = \"\"\"(**GOAL**  \n",
    "This is a course planning assistant exercise in which you, the AI Planner Co-Pilot, help a student navigate their academic journey...\n",
    "[Insert full prompt here, unchanged]\"\"\"\n",
    "\n",
    "# Initialize retriever as global so it updates after uploads\n",
    "retriever = None\n",
    "\n",
    "# Build retriever from uploaded files\n",
    "def build_retriever(uploaded_files):\n",
    "    docs = []\n",
    "    for file in uploaded_files:\n",
    "        loader = TextLoader(file.name, encoding=\"utf-8\")\n",
    "        docs.extend(loader.load())\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    split_docs = splitter.split_documents(docs)\n",
    "    vectorstore = Chroma.from_documents(split_docs, embedding=embedding_model)\n",
    "    return vectorstore.as_retriever()\n",
    "\n",
    "# Main chat function\n",
    "def predict(message, history, files):\n",
    "    global retriever\n",
    "    if retriever is None and files:\n",
    "        retriever = build_retriever(files)\n",
    "\n",
    "    context_docs = retriever.invoke(message) if retriever else []\n",
    "    context = \"\\n\".join([doc.page_content for doc in context_docs])\n",
    "    full_prompt = f\"{system_prompt}\\n\\n{context}\\n\\nQuestion: {message}\"\n",
    "    \n",
    "    history_openai_format = []\n",
    "    for human, assistant in history:\n",
    "        history_openai_format.append({\"role\": \"user\", \"content\": human})\n",
    "        history_openai_format.append({\"role\": \"assistant\", \"content\": assistant})\n",
    "    history_openai_format.append({\"role\": \"user\", \"content\": full_prompt})\n",
    "\n",
    "    response = llm.client.chat.completions.create(\n",
    "        model=llm.model_name,\n",
    "        messages=history_openai_format,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    partial_message = \"\"\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            partial_message += chunk.choices[0].delta.content\n",
    "            yield partial_message\n",
    "\n",
    "# Gradio app\n",
    "gr.ChatInterface(\n",
    "    fn=predict,\n",
    "    additional_inputs=[\n",
    "        gr.File(file_types=[\".txt\"], label=\"Upload course files\", file_count=\"multiple\")\n",
    "    ],\n",
    "    title=\"üéì Course Planner Co-Pilot\",\n",
    "    description=\"Upload syllabi or planning documents on the left and chat with the AI about your course plan.\"\n",
    ").queue().launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb71d16d-a3bc-4fa8-b099-e8162aa06eff",
   "metadata": {},
   "source": [
    "### Setup a query on a base model with no recent DP knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4138447e-64da-4fdd-871f-5d5ddfa46358",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can you help me plan my Data Science Minor?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6cbb88d5-5ba0-4e99-9fde-1cca4086aaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\":prompt},\n",
    "    {\"role\": \"user\", \"content\": query},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d8442a4-e38e-4c99-887c-5786feba88d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m      2\u001b[0m   model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m   messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m      4\u001b[0m   max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"llama2\",\n",
    "  messages=messages,\n",
    "  max_tokens=300\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c58a116-994f-490f-a42b-4c8e2612916b",
   "metadata": {},
   "source": [
    "### Manually include relevant document as context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6d2a97-63fa-44e3-a035-11d7b95b1652",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_doc = open('documents/dp_sports/penn-mens-swim-ncaa-championships-matt-fallon-recap.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c849c0c-68a4-4a87-a496-2196a4133eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Answer questions ONLY if you know the answer\"},\n",
    "    {\"role\": \"user\", \"content\": dp_doc},\n",
    "    {\"role\": \"user\", \"content\": query\n",
    "    },\n",
    "   \n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211ca31c-1d15-45b1-96fa-d67760b67fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"llama2\",\n",
    "  messages=messages,\n",
    "  max_tokens=300\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86d8fa1-4c68-4a31-84c0-4b620d47bd04",
   "metadata": {},
   "source": [
    "## `langchain` RAG example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188afb83-cb3e-4c5b-b3d7-035dccf78f2a",
   "metadata": {},
   "source": [
    "### 1. Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8db2410-5aed-4e93-a822-a941394672f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader('./documents', glob=\"dp_sports/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89187367-1246-431d-85c0-fc3865fe9351",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()\n",
    "print(f'Loaded {len(docs)} documents')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee35f32e-2765-45de-88de-5bc0765eeff8",
   "metadata": {},
   "source": [
    "### 2. Split into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c4d751-f15a-4c9d-8474-b0f21fb183b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, \n",
    "    add_start_index=True,\n",
    "    separators=[\" \"]\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88125853-a3be-4838-a012-070dd89ef662",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fe16c1-9ee8-4016-b1d8-998067ab9ea3",
   "metadata": {},
   "source": [
    "* Looking at chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2358286d-1874-4ec5-9ea6-18f988760070",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a67e36-9462-42db-a9ca-49400b812e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd1ed0a-ad52-4cd1-b99f-bc7c4c3ae85a",
   "metadata": {},
   "source": [
    "### Load chunks into vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177734c6-07e2-483d-9f56-f02ea4a4cddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=all_splits, \n",
    "                                    embedding=OpenAIEmbeddings(),\n",
    "                                    persist_directory='./chroma_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a72e169-37bf-441d-b1e0-e918e67e3472",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", \n",
    "                                     search_kwargs={\"k\": 6})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d42cb21-d72c-4e8a-a6f9-70d8d880296a",
   "metadata": {},
   "source": [
    "#### Example query on vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43f5c88-6a39-4362-991c-603abf3d6cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e62a036-aab4-4d38-b5b6-766890da7c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(query)\n",
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91195977-90fa-4884-aeda-708c35c50f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_docs_and_scores = vectorstore.similarity_search_with_relevance_scores(query,\n",
    "                                                                                k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a840b93-107b-47df-a339-213a210debec",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = []\n",
    "for chunk, score in relevance_docs_and_scores:\n",
    "    chunks.append({'content': chunk.page_content,\n",
    "                   'similarity_score': score,\n",
    "                   'document': chunk.metadata['source']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9117e99c-72d8-4c77-96f4-d67555e77b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac9304b-043d-4419-8a79-b475a9e0b4cb",
   "metadata": {},
   "source": [
    "## `langchain` RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57bfbc7-b6d9-4bc4-a0c4-5bb51c0a477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1552d717-7137-4dcf-95c5-3b4a551eb73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78799535-c0b3-4ba6-b9ea-365a78f07781",
   "metadata": {},
   "source": [
    "* Setup LLM component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89b387e-97fa-4ed8-b197-70b0faea9106",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama = Ollama(\n",
    "        base_url=\"http://10.30.16.100:11434\",\n",
    "        model=\"llama2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5861b5-b549-496a-8ef5-47b29b59a4e2",
   "metadata": {},
   "source": [
    "### Setup chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afe5683-47b6-4506-8cf6-cf92468d6edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_result = (retriever | format_docs)\n",
    "\n",
    "norag_chain = (\n",
    "    prompt\n",
    "    | ollama\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | ollama\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2339993-7ba2-4397-9774-40c707d68e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query2 = \"Which team ended Penn's baseball win streak?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b43e69c-adde-4c3c-89fc-f303c288e512",
   "metadata": {},
   "source": [
    "#### with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04fc12be-be10-441d-bd4b-76d7e11b77cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rag_chain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrag_chain\u001b[49m\u001b[38;5;241m.\u001b[39minvoke(query)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rag_chain' is not defined"
     ]
    }
   ],
   "source": [
    "rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8750abb9-e9ca-48ef-aa84-7a26b08a5427",
   "metadata": {},
   "source": [
    "#### No RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7d08e37-289c-4f1f-885d-9dca919564c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'norag_chain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnorag_chain\u001b[49m\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: query, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'norag_chain' is not defined"
     ]
    }
   ],
   "source": [
    "norag_chain.invoke({\"question\": query, 'context': ''})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cdfdab-bb42-4399-8669-e361c741cf5e",
   "metadata": {},
   "source": [
    "#### Looking at the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444df699-a217-4c7e-a1f3-70c82cc64170",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rag_result.invoke(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d96b6b-9627-4748-a970-aaad6e79d080",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4df1337-8f91-4756-b7e5-4a7dc3ce7c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {}
  },
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
