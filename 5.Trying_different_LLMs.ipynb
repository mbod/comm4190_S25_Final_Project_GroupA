{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c73f73a8-02fc-4b8a-beae-b24120c1d419",
   "metadata": {},
   "source": [
    "# Trying Different LLMs\n",
    "\n",
    "### Prompting different LLM's to become Penn CAS Co-pilots and testing there ability\n",
    "\n",
    "\n",
    "* CAN YOU ADD MORE DETAIL TO WHAT THIS NOTEBOOK IS DOING\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a4cd4b-0403-4d19-8131-bb0166225015",
   "metadata": {},
   "source": [
    "\n",
    "## 🧪 **Trying Different LLMs: External Benchmarking of the Planner Co-Pilot**\n",
    "\n",
    "As explored in Notebooks 1–4, our AI Planner Co-Pilot was designed with a structured understanding of Penn’s curriculum, allowing it to support multi-program students through complex academic trajectories. In this section, we evaluate how three public LLMs—***Claude, Gemini, and Deepseek***—perform on the same advising scenario.\n",
    "\n",
    "Rather than rebuild redundant scaffolding, we begin here where the prior notebooks leave off: having implemented a prompt interface, structured user input, and course requirement ingestion, we now shift to benchmarking our system against alternative language models on a standardized advising task.\n",
    "\n",
    "\n",
    "### 🎯 Evaluation Setup\n",
    "***All models were prompted with the same setup:***\n",
    "\n",
    "- A system message establishing their role as the AI Planner Co-Pilot\n",
    "\n",
    "- The same CSV course requirement files for:\n",
    "\n",
    "    - Economics major\n",
    "\n",
    "    - Data Science & Analytics minor\n",
    " \n",
    "    - Communications major (Data & Network Science)\n",
    " \n",
    "    - French & Francophone Studies major\n",
    " \n",
    "    - CIMS major (Cinema & Media Studies)\n",
    "\n",
    "- A student profile:\n",
    "\n",
    "    - Sophomore in Spring semester\n",
    " \n",
    "    - Economics major\n",
    "\n",
    "    - Considering DSA minor\n",
    "\n",
    "    - Planning to study abroad in Junior Spring\n",
    "\n",
    "    - Preference for ≤ 5 CUs per semester\n",
    "\n",
    "***Models were evaluated on:***\n",
    "\n",
    "- Factual accuracy (course names, prerequisites, sequencing)\n",
    "\n",
    "- Plan feasibility under credit/time constraints\n",
    "\n",
    "- Response adaptability (to user preferences, tangents, schedule revisions)\n",
    "\n",
    "- Long-context retention\n",
    "\n",
    "- Robustness to additional CSV uploads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af400726-76e4-4684-b74c-4058251cf68a",
   "metadata": {},
   "source": [
    "## 1\n",
    "\n",
    "### 🧠 Claude\n",
    "<img src=\"img/imag1.png\" width=\"50%\"/>\n",
    "\n",
    "Claude offered the **most academically grounded output** of the public models tested. It correctly parsed requirement structures, avoided hallucinations in early outputs, and was particularly effective at calculating feasibility under load constraints.\n",
    "\n",
    "#### 🧩 Excerpt D (Strategic Constraint Handling):\n",
    "<img src=\"img/imag2.png\" width=\"50%\"/>\n",
    "\n",
    "🧠 **Analysis:**\n",
    "Claude excels in **realistic constraint articulation**. Rather than generically agree with user requests, it imposes logical friction—clearly articulating tradeoffs between ambition and feasibility.\n",
    "\n",
    "#### 🗃️ Excerpt E (Cross-listed Courses):\n",
    "<img src=\"img/imag3.png\" width=\"50%\"/>\n",
    "\n",
    "💡 **Analysis:**\n",
    "Claude is the only model that **proactively suggests cross-listed courses**, showing familiarity with real catalog structures. This ability to exploit dual-counting between majors (not just within one department) is rare among LLMs and demonstrates a nuanced understanding of Penn’s offerings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8273a27-d81c-4ceb-a777-597c65b32136",
   "metadata": {},
   "source": [
    "## 2\n",
    "\n",
    "### 🌐 Gemini\n",
    "<img src=\"img/imag4.png\" width=\"50%\"/>\n",
    "\n",
    "Gemini produced fluent, structured advice but suffered from **severe factual unreliability**. Despite its large token window (~1M), it hallucinated multiple non-existent courses, mislabeled common requirements, and failed to raise feasibility concerns.\n",
    "\n",
    "#### 🧩 Excerpt D (Hallucinated Course 1):\n",
    "<img src=\"img/imag5.png\" width=\"50%\"/>\n",
    "\n",
    "🚨 **Analysis:**\n",
    "This is a **fictional course**. It sounds plausible, but does not exist in Penn’s course catalog. Gemini generated it without being prompted to extrapolate, suggesting a bias toward sounding helpful over being factual.\n",
    "\n",
    "#### 🧩 Excerpt E (Dismissive Feasibility):\n",
    "“Sure! Adding a French major on top of Econ, CIMS, and DSA is ambitious, but absolutely possible in your final three semesters.”\n",
    "\n",
    "❌ **Analysis:**\n",
    "This excerpt highlights a **failure to gate-check logic**. Unlike Claude, Gemini shows no cost-benefit awareness—it green-lights structurally impossible combinations without acknowledging credit caps or term limits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8d460a-7d7e-440f-9055-c5252f2d661d",
   "metadata": {},
   "source": [
    "## 3 \n",
    "### 🔬 Deepseek\n",
    "Deepseek began strongly with user-centered prompts and structurally sound responses, but gradually drifted into **hallucination and sequencing errors**. It was especially vulnerable in longer conversations or when prompted to include additional CSVs.\n",
    "\n",
    "#### 🧩 Excerpt D (Misordered Pathway):\n",
    "<img src=\"img/imag7.png\" width=\"50%\"/>\n",
    "\n",
    "📉 **Analysis:**\n",
    "This course does not exist at Penn (The code does exist but as another class).\n",
    "\n",
    "#### 🧩 Excerpt E (Applied Focus Prompt):\n",
    "<img src=\"img/imag8.png\" width=\"50%\"/>\n",
    "\n",
    "\n",
    "🧠 **Analysis:**\n",
    "While Deepseek struggled with structure, it excelled at simulation. Asking this question shows that it understood the subjective nature of course planning. No other model attempted to tailor the plan to academic style preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd1f82b-1521-4dc5-becf-620e3540957e",
   "metadata": {},
   "source": [
    "### ⚙️ Additional Findings\n",
    "#### 🔄 Scalability Failure with Additional CSVs\n",
    "When we attempted to **expand the planner’s capabilities by adding more majors/minors** (e.g., adding History, PPE, or Visual Studies CSVs), **all three models broke down**. Specifically:\n",
    "\n",
    "- **Claude** became verbose but non-specific, often defaulting to vague reassurances like “that could be possible.”\n",
    "\n",
    "- **Gemini** increased hallucination frequency dramatically—suggesting entire sets of fake courses.\n",
    "\n",
    "- **Deepseek** collapsed into repetition loops or invented course numbers across departments.\n",
    "\n",
    "🧠 **Implication:** None of the public LLMs can **scale beyond 3–4 CSV-loaded programs** without losing structural integrity. This directly contrasts with our planner’s architecture, which allows modular ingestion—simply adding a CSV enables instant support for a new major/minor without prompt degradation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 🧠 Subsection: Long-Context Degradation\n",
    "In tests where interactions extended past 10+ turns:\n",
    "\n",
    "- **Claude** maintained the best memory but began simplifying its reasoning\n",
    "\n",
    "- **Gemini** exhibited increasing hallucination and redundancy\n",
    "\n",
    "- **Deepseek** began repeating itself and suggesting inconsistent pathways (e.g., recommending a course both in Junior Spring and Study Abroad)\n",
    "\n",
    "📉 This suggests that none of the models maintain **multi-turn consistency** when the academic plan becomes deeply entangled—especially across semesters, constraints, and triple-major logic.\n",
    "\n",
    "\n",
    "\n",
    "### 🎓 Subsection: Policy Awareness\n",
    "Only **Claude** showed even partial awareness of Penn’s credit cap, double-counting rules, or course availability constraints.\n",
    "\n",
    "Gemini and Deepseek consistently:\n",
    "\n",
    "- Recommended 6-course semesters without warning\n",
    "\n",
    "- Ignored the fact that not all courses are offered every semester\n",
    "\n",
    "- Failed to respect prerequisites in course ordering\n",
    "\n",
    "⚠️ These oversights could have serious consequences in real advising situations, reinforcing the need for hard-coded policy integration in academic LLM tools.\n",
    "\n",
    "\n",
    "\n",
    "## 📍 **Conclusion**\n",
    "Across the board, Claude was the **most structured and realistic**, though still imperfect. Gemini was stylistically strong but **deeply unreliable**, and Deepseek showed the most promise in student-centered advising while **struggling with factual control.**\n",
    "\n",
    "The clear limitation of all three: **scalability, consistency, and trustworthiness** degrade sharply under pressure. In contrast to our own Co-Pilot, which scales linearly with CSV additions, the generalist models falter.\n",
    "\n",
    "This reinforces the broader point: LLMs without hard-coded curriculum logic, memory control, and institutional policy grounding may be great chat partners—but they're not academic co-pilots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3564860-cc35-44d8-9921-cb169587e641",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
