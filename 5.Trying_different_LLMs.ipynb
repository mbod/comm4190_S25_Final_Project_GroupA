{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c73f73a8-02fc-4b8a-beae-b24120c1d419",
   "metadata": {},
   "source": [
    "# Comparing Different LLMs: Optimizing AI for Course Planning\n",
    "\n",
    "### Prompting different LLM's to become Penn CAS Co-pilots and testing there ability\n",
    "\n",
    "This notebook includes a comparison of several large language models (LLMs) and evalutes their appropriateness for helping the Course Planner Co-Pilot's academic advising capabilities. We test how well each model understands and responds to academic planning queries, respects prompt boundaries, and handles conversation focus in controlled experiments. These experiments help us demonstrate that the Co-Pilot has the ability to create recommendations appropriate to students' academic goals.\n",
    "\n",
    "This notebook also investigates how the various models handle off-topic requests  designed to be distractions. These experiments are critical to test our prompt engineering and how the models deal with detecting and rejecting useless asks. The insights from the experiments help us understand which LLMs possess the ideal level of responsiveness at the cost of staying on task, making them a better candidate for being a Co-pilot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a4cd4b-0403-4d19-8131-bb0166225015",
   "metadata": {},
   "source": [
    "\n",
    "## üß™ **Trying Different LLMs: External Benchmarking of the Planner Co-Pilot**\n",
    "\n",
    "As explored in Notebooks 1‚Äì4, our AI Planner Co-Pilot was designed with a structured understanding of Penn‚Äôs curriculum, allowing it to support multi-program students through complex academic trajectories. In this section, we evaluate how three public LLMs‚Äî***Claude, Gemini, and Deepseek***‚Äîperform on the same advising scenario.\n",
    "\n",
    "Rather than rebuild redundant scaffolding, we begin here where the prior notebooks leave off: having implemented a prompt interface, structured user input, and course requirement ingestion, we now shift to benchmarking our system against alternative language models on a standardized advising task.\n",
    "\n",
    "\n",
    "### üéØ Evaluation Setup\n",
    "***All models were prompted with the same setup:***\n",
    "\n",
    "- A system message establishing their role as the AI Planner Co-Pilot\n",
    "\n",
    "- The same CSV course requirement files for:\n",
    "\n",
    "    - Economics major\n",
    "\n",
    "    - Data Science & Analytics minor\n",
    " \n",
    "    - Communications major (Data & Network Science)\n",
    " \n",
    "    - French & Francophone Studies major\n",
    " \n",
    "    - CIMS major (Cinema & Media Studies)\n",
    "\n",
    "- A student profile:\n",
    "\n",
    "    - Sophomore in Spring semester\n",
    " \n",
    "    - Economics major\n",
    "\n",
    "    - Considering DSA minor\n",
    "\n",
    "    - Planning to study abroad in Junior Spring\n",
    "\n",
    "    - Preference for ‚â§ 5 CUs per semester\n",
    "\n",
    "***Models were evaluated on:***\n",
    "\n",
    "- Factual accuracy (course names, prerequisites, sequencing)\n",
    "\n",
    "- Plan feasibility under credit/time constraints\n",
    "\n",
    "- Response adaptability (to user preferences, tangents, schedule revisions)\n",
    "\n",
    "- Long-context retention\n",
    "\n",
    "- Robustness to additional CSV uploads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af400726-76e4-4684-b74c-4058251cf68a",
   "metadata": {},
   "source": [
    "## 1\n",
    "\n",
    "### üß† Claude\n",
    "<img src=\"img/imag1.png\" width=\"50%\"/>\n",
    "\n",
    "Claude offered the **most academically grounded output** of the public models tested. It correctly parsed requirement structures, avoided hallucinations in early outputs, and was particularly effective at calculating feasibility under load constraints.\n",
    "\n",
    "#### üß© Excerpt D (Strategic Constraint Handling):\n",
    "<img src=\"img/imag2.png\" width=\"50%\"/>\n",
    "\n",
    "üß† **Analysis:**\n",
    "Claude excels in **realistic constraint articulation**. Rather than generically agree with user requests, it imposes logical friction‚Äîclearly articulating tradeoffs between ambition and feasibility.\n",
    "\n",
    "#### üóÉÔ∏è Excerpt E (Cross-listed Courses):\n",
    "<img src=\"img/imag3.png\" width=\"50%\"/>\n",
    "\n",
    "üí° **Analysis:**\n",
    "Claude is the only model that **proactively suggests cross-listed courses**, showing familiarity with real catalog structures. This ability to exploit dual-counting between majors (not just within one department) is rare among LLMs and demonstrates a nuanced understanding of Penn‚Äôs offerings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8273a27-d81c-4ceb-a777-597c65b32136",
   "metadata": {},
   "source": [
    "## 2\n",
    "\n",
    "### üåê Gemini\n",
    "<img src=\"img/imag4.png\" width=\"50%\"/>\n",
    "\n",
    "Gemini produced fluent, structured advice but suffered from **severe factual unreliability**. Despite its large token window (~1M), it hallucinated multiple non-existent courses, mislabeled common requirements, and failed to raise feasibility concerns.\n",
    "\n",
    "#### üß© Excerpt D (Hallucinated Course 1):\n",
    "<img src=\"img/imag5.png\" width=\"50%\"/>\n",
    "\n",
    "üö® **Analysis:**\n",
    "This is a **fictional course**. It sounds plausible, but does not exist in Penn‚Äôs course catalog. Gemini generated it without being prompted to extrapolate, suggesting a bias toward sounding helpful over being factual.\n",
    "\n",
    "#### üß© Excerpt E (Dismissive Feasibility):\n",
    "‚ÄúSure! Adding a French major on top of Econ, CIMS, and DSA is ambitious, but absolutely possible in your final three semesters.‚Äù\n",
    "\n",
    "‚ùå **Analysis:**\n",
    "This excerpt highlights a **failure to gate-check logic**. Unlike Claude, Gemini shows no cost-benefit awareness‚Äîit green-lights structurally impossible combinations without acknowledging credit caps or term limits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8d460a-7d7e-440f-9055-c5252f2d661d",
   "metadata": {},
   "source": [
    "## 3 \n",
    "### üî¨ Deepseek\n",
    "Deepseek began strongly with user-centered prompts and structurally sound responses, but gradually drifted into **hallucination and sequencing errors**. It was especially vulnerable in longer conversations or when prompted to include additional CSVs.\n",
    "\n",
    "#### üß© Excerpt D (Misordered Pathway):\n",
    "<img src=\"img/imag7.png\" width=\"50%\"/>\n",
    "\n",
    "üìâ **Analysis:**\n",
    "This course does not exist at Penn (The code does exist but as another class).\n",
    "\n",
    "#### üß© Excerpt E (Applied Focus Prompt):\n",
    "<img src=\"img/imag8.png\" width=\"50%\"/>\n",
    "\n",
    "\n",
    "üß† **Analysis:**\n",
    "While Deepseek struggled with structure, it excelled at simulation. Asking this question shows that it understood the subjective nature of course planning. No other model attempted to tailor the plan to academic style preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd1f82b-1521-4dc5-becf-620e3540957e",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Additional Findings\n",
    "#### üîÑ Scalability Failure with Additional CSVs\n",
    "When we attempted to **expand the planner‚Äôs capabilities by adding more majors/minors** (e.g., adding History, PPE, or Visual Studies CSVs), **all three models broke down**. Specifically:\n",
    "\n",
    "- **Claude** became verbose but non-specific, often defaulting to vague reassurances like ‚Äúthat could be possible.‚Äù\n",
    "\n",
    "- **Gemini** increased hallucination frequency dramatically‚Äîsuggesting entire sets of fake courses.\n",
    "\n",
    "- **Deepseek** collapsed into repetition loops or invented course numbers across departments.\n",
    "\n",
    "üß† **Implication:** None of the public LLMs can **scale beyond 3‚Äì4 CSV-loaded programs** without losing structural integrity. This directly contrasts with our planner‚Äôs architecture, which allows modular ingestion‚Äîsimply adding a CSV enables instant support for a new major/minor without prompt degradation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### üß† Subsection: Long-Context Degradation\n",
    "In tests where interactions extended past 10+ turns:\n",
    "\n",
    "- **Claude** maintained the best memory but began simplifying its reasoning\n",
    "\n",
    "- **Gemini** exhibited increasing hallucination and redundancy\n",
    "\n",
    "- **Deepseek** began repeating itself and suggesting inconsistent pathways (e.g., recommending a course both in Junior Spring and Study Abroad)\n",
    "\n",
    "üìâ This suggests that none of the models maintain **multi-turn consistency** when the academic plan becomes deeply entangled‚Äîespecially across semesters, constraints, and triple-major logic.\n",
    "\n",
    "\n",
    "\n",
    "### üéì Subsection: Policy Awareness\n",
    "Only **Claude** showed even partial awareness of Penn‚Äôs credit cap, double-counting rules, or course availability constraints.\n",
    "\n",
    "Gemini and Deepseek consistently:\n",
    "\n",
    "- Recommended 6-course semesters without warning\n",
    "\n",
    "- Ignored the fact that not all courses are offered every semester\n",
    "\n",
    "- Failed to respect prerequisites in course ordering\n",
    "\n",
    "‚ö†Ô∏è These oversights could have serious consequences in real advising situations, reinforcing the need for hard-coded policy integration in academic LLM tools.\n",
    "\n",
    "\n",
    "\n",
    "## üìç **Conclusion**\n",
    "Across the board, Claude was the **most structured and realistic**, though still imperfect. Gemini was stylistically strong but **deeply unreliable**, and Deepseek showed the most promise in student-centered advising while **struggling with factual control.**\n",
    "\n",
    "The clear limitation of all three: **scalability, consistency, and trustworthiness** degrade sharply under pressure. In contrast to our own Co-Pilot, which scales linearly with CSV additions, the generalist models falter.\n",
    "\n",
    "This reinforces the broader point: LLMs without hard-coded curriculum logic, memory control, and institutional policy grounding may be great chat partners‚Äîbut they're not academic co-pilots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3564860-cc35-44d8-9921-cb169587e641",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
